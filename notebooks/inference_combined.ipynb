{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:16:55.726022Z",
     "start_time": "2023-10-22T00:16:55.710022Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import DPMSolverMultistepScheduler\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "from models.combined_pipeline import CombinedPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:16:57.843701Z",
     "start_time": "2023-10-22T00:16:55.727023Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3583c20e7e974e76b9a6ddc5a692976e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = CombinedPipeline.from_pretrained('../models/weights/img2img-painting', torch_dtype=torch.float16, variant='fp16', use_safetensors=True)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "lora_model_path = '../models/weights/img2img-painting-lora/pytorch_lora_weights.safetensors'\n",
    "# lora_model_path = '../models/weights/_/models/weights/img2img-painting-lora/checkpoint-500/pytorch_model.bin'\n",
    "# pipe.unet.load_attn_procs(lora_model_path)\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:16:57.859703Z",
     "start_time": "2023-10-22T00:16:57.846701Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_images(images, path=''):\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(path + f'image{i}.png')\n",
    "\n",
    "def create_img(source_image, target_image):\n",
    "    text = ['shirt', 'pants', 'shorts', 'shoes', 't-shirt',\n",
    "            'sweatshirt', \"cowboy boot\", \"cowboy hat\",\n",
    "            'crash helmet', 'cardigan', 'blouse', 'jacket', 'jeans',]\n",
    "    condition = ['head, neck', 'hair', 'face', 'hand',]\n",
    "    return pipe(source_image, target_image,\n",
    "                source_prompt=text,\n",
    "                source_negative_prompt=condition,\n",
    "                target_prompt=text,\n",
    "                target_negative_prompt=condition,\n",
    "                guidance_scale=9, num_inference_steps=100,\n",
    "                mask_threshold=0.4,\n",
    "                cross_attention_kwargs={\"scale\": 0.5},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:16:57.877703Z",
     "start_time": "2023-10-22T00:16:57.859703Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_folder = '../cache/in/'\n",
    "output_folder = '../cache/out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:48:53.413536Z",
     "start_time": "2023-10-22T00:48:53.229537Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "DATASET_DIR = 'SaffalPoosh/deepFashion-with-masks'\n",
    "dataset = load_from_disk('../data/dataset/train/')\n",
    "start = 119\n",
    "count = 5\n",
    "save_images(dataset['train'][start: start + count]['images'], path=input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:26:52.262749Z",
     "start_time": "2023-10-22T00:16:59.046118Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671652730b6d4e9a8737fd303b50ca6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32mc:\\Folder\\WorkProjects\\Workspace\\image-merging\\notebooks\\inference_combined.ipynb Cell 6\u001B[0m line \u001B[0;36m1\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m \u001B[39mfor\u001B[39;00m j \u001B[39min\u001B[39;00m \u001B[39mrange\u001B[39m(i \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m, \u001B[39mlen\u001B[39m(images)):\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m     images[i], images[j] \u001B[39m=\u001B[39m images[i]\u001B[39m.\u001B[39mresize((\u001B[39m512\u001B[39m, \u001B[39m512\u001B[39m)), images[j]\u001B[39m.\u001B[39mresize((\u001B[39m512\u001B[39m, \u001B[39m512\u001B[39m))\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m     generated_images\u001B[39m.\u001B[39mextend([\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m         make_image_grid([\u001B[39m*\u001B[39mcreate_img(images[i], images[j]),\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001B[0m                          images[i], images[j]], \u001B[39m3\u001B[39m, \u001B[39m2\u001B[39m),\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001B[0m         make_image_grid([\u001B[39m*\u001B[39mcreate_img(images[j], images[i]),\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m                          images[j], images[i]], \u001B[39m3\u001B[39m, \u001B[39m2\u001B[39m)\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001B[0m     ])\n",
      "\u001B[1;32mc:\\Folder\\WorkProjects\\Workspace\\image-merging\\notebooks\\inference_combined.ipynb Cell 6\u001B[0m line \u001B[0;36m1\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m text \u001B[39m=\u001B[39m [\u001B[39m'\u001B[39m\u001B[39mshirt\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mpants\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mshorts\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mshoes\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mt-shirt\u001B[39m\u001B[39m'\u001B[39m,\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m         \u001B[39m'\u001B[39m\u001B[39msweatshirt\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mcowboy boot\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mcowboy hat\u001B[39m\u001B[39m\"\u001B[39m,\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m         \u001B[39m'\u001B[39m\u001B[39mcrash helmet\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mcardigan\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mblouse\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mjacket\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mjeans\u001B[39m\u001B[39m'\u001B[39m,]\n\u001B[0;32m      <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m condition \u001B[39m=\u001B[39m [\u001B[39m'\u001B[39m\u001B[39mhead, neck\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mhair\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mface\u001B[39m\u001B[39m'\u001B[39m, \u001B[39m'\u001B[39m\u001B[39mhand\u001B[39m\u001B[39m'\u001B[39m,]\n\u001B[1;32m---> <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m \u001B[39mreturn\u001B[39;00m pipe(source_image, target_image,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001B[0m             source_prompt\u001B[39m=\u001B[39;49mtext,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001B[0m             source_negative_prompt\u001B[39m=\u001B[39;49mcondition,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001B[0m             target_prompt\u001B[39m=\u001B[39;49mtext,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001B[0m             target_negative_prompt\u001B[39m=\u001B[39;49mcondition,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001B[0m             guidance_scale\u001B[39m=\u001B[39;49m\u001B[39m9\u001B[39;49m, num_inference_steps\u001B[39m=\u001B[39;49m\u001B[39m100\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001B[0m             mask_threshold\u001B[39m=\u001B[39;49m\u001B[39m0.4\u001B[39;49m,\n\u001B[0;32m     <a href='vscode-notebook-cell:/c%3A/Folder/WorkProjects/Workspace/image-merging/notebooks/inference_combined.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001B[0m             cross_attention_kwargs\u001B[39m=\u001B[39;49m{\u001B[39m\"\u001B[39;49m\u001B[39mscale\u001B[39;49m\u001B[39m\"\u001B[39;49m: \u001B[39m0.5\u001B[39;49m},)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[39m@functools\u001B[39m\u001B[39m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mdecorate_context\u001B[39m(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[39mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[39mreturn\u001B[39;00m func(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\models\\combined_pipeline.py:214\u001B[0m, in \u001B[0;36mCombinedPipeline.__call__\u001B[1;34m(self, source_image, target_image, source_mask_image, target_mask_image, source_prompt, source_negative_prompt, target_prompt, target_negative_prompt, num_inference_steps, guidance_scale, num_images_per_prompt, height, width, strength, num_maps_per_mask, mask_threshold, generator, output_type, callback, callback_steps, return_dict, cross_attention_kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m source_image_embeds, source_negative_image_embeds \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mencode_image(source_image,\n\u001B[0;32m    206\u001B[0m                                                                       device,\n\u001B[0;32m    207\u001B[0m                                                                       batch_size,\n\u001B[0;32m    208\u001B[0m                                                                       num_images_per_prompt)\n\u001B[0;32m    210\u001B[0m target_image_embeds, target_negative_image_embeds \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mencode_image(masked_target_image[\u001B[39m0\u001B[39m],\n\u001B[0;32m    211\u001B[0m                                                                       device,\n\u001B[0;32m    212\u001B[0m                                                                       batch_size,\n\u001B[0;32m    213\u001B[0m                                                                       num_images_per_prompt)\n\u001B[1;32m--> 214\u001B[0m outputs \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mdecoder_pipe(\n\u001B[0;32m    215\u001B[0m     source_image,\n\u001B[0;32m    216\u001B[0m     source_image_embeds,\n\u001B[0;32m    217\u001B[0m     target_image_embeds,\n\u001B[0;32m    218\u001B[0m     source_mask_image,\n\u001B[0;32m    219\u001B[0m     source_negative_image_embeds\u001B[39m=\u001B[39;49msource_negative_image_embeds,\n\u001B[0;32m    220\u001B[0m     target_negative_image_embeds\u001B[39m=\u001B[39;49mtarget_negative_image_embeds,\n\u001B[0;32m    221\u001B[0m     width\u001B[39m=\u001B[39;49mwidth,\n\u001B[0;32m    222\u001B[0m     height\u001B[39m=\u001B[39;49mheight,\n\u001B[0;32m    223\u001B[0m     num_inference_steps\u001B[39m=\u001B[39;49mnum_inference_steps,\n\u001B[0;32m    224\u001B[0m     strength\u001B[39m=\u001B[39;49mstrength,\n\u001B[0;32m    225\u001B[0m     generator\u001B[39m=\u001B[39;49mgenerator,\n\u001B[0;32m    226\u001B[0m     guidance_scale\u001B[39m=\u001B[39;49mguidance_scale,\n\u001B[0;32m    227\u001B[0m     output_type\u001B[39m=\u001B[39;49moutput_type,\n\u001B[0;32m    228\u001B[0m     callback\u001B[39m=\u001B[39;49mcallback,\n\u001B[0;32m    229\u001B[0m     callback_steps\u001B[39m=\u001B[39;49mcallback_steps,\n\u001B[0;32m    230\u001B[0m     return_dict\u001B[39m=\u001B[39;49mreturn_dict,\n\u001B[0;32m    231\u001B[0m )\n\u001B[0;32m    233\u001B[0m \u001B[39m# Offload all models\u001B[39;00m\n\u001B[0;32m    234\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmaybe_free_model_hooks()\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[39m@functools\u001B[39m\u001B[39m.\u001B[39mwraps(func)\n\u001B[0;32m    113\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mdecorate_context\u001B[39m(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m    114\u001B[0m     \u001B[39mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 115\u001B[0m         \u001B[39mreturn\u001B[39;00m func(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\models\\img2img_inpaint_pipeline.py:225\u001B[0m, in \u001B[0;36mImg2ImgInpaintPipeline.__call__\u001B[1;34m(self, source_image, source_image_embeds, target_image_embeds, mask_image, source_negative_image_embeds, target_negative_image_embeds, height, width, num_inference_steps, strength, guidance_scale, num_images_per_prompt, generator, latents, output_type, callback, callback_steps, return_dict)\u001B[0m\n\u001B[0;32m    223\u001B[0m \u001B[39m# post-processing\u001B[39;00m\n\u001B[0;32m    224\u001B[0m latents \u001B[39m=\u001B[39m mask_image[:\u001B[39m1\u001B[39m] \u001B[39m*\u001B[39m source_image[:\u001B[39m1\u001B[39m] \u001B[39m+\u001B[39m (\u001B[39m1\u001B[39m \u001B[39m-\u001B[39m mask_image[:\u001B[39m1\u001B[39m]) \u001B[39m*\u001B[39m latents\n\u001B[1;32m--> 225\u001B[0m source_image \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmovq\u001B[39m.\u001B[39;49mdecode(latents, force_not_quantize\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m)[\u001B[39m\"\u001B[39m\u001B[39msample\u001B[39m\u001B[39m\"\u001B[39m]\n\u001B[0;32m    227\u001B[0m \u001B[39m# Offload all models\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mmaybe_free_model_hooks()\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\diffusers\\utils\\accelerate_utils.py:45\u001B[0m, in \u001B[0;36mapply_forward_hook.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mwrapper\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m     44\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mhasattr\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39m_hf_hook\u001B[39m\u001B[39m\"\u001B[39m) \u001B[39mand\u001B[39;00m \u001B[39mhasattr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_hf_hook, \u001B[39m\"\u001B[39m\u001B[39mpre_forward\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[1;32m---> 45\u001B[0m         \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_hf_hook\u001B[39m.\u001B[39;49mpre_forward(\u001B[39mself\u001B[39;49m)\n\u001B[0;32m     46\u001B[0m     \u001B[39mreturn\u001B[39;00m method(\u001B[39mself\u001B[39m, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\accelerate\\hooks.py:577\u001B[0m, in \u001B[0;36mCpuOffload.pre_forward\u001B[1;34m(self, module, *args, **kwargs)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mpre_forward\u001B[39m(\u001B[39mself\u001B[39m, module, \u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs):\n\u001B[0;32m    576\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mprev_module_hook \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[1;32m--> 577\u001B[0m         \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mprev_module_hook\u001B[39m.\u001B[39;49moffload()\n\u001B[0;32m    578\u001B[0m     module\u001B[39m.\u001B[39mto(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mexecution_device)\n\u001B[0;32m    579\u001B[0m     \u001B[39mreturn\u001B[39;00m send_to_device(args, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mexecution_device), send_to_device(kwargs, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mexecution_device)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\accelerate\\hooks.py:593\u001B[0m, in \u001B[0;36mUserCpuOffloadHook.offload\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    592\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39moffload\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[1;32m--> 593\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mhook\u001B[39m.\u001B[39;49minit_hook(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mmodel)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\accelerate\\hooks.py:573\u001B[0m, in \u001B[0;36mCpuOffload.init_hook\u001B[1;34m(self, module)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39minit_hook\u001B[39m(\u001B[39mself\u001B[39m, module):\n\u001B[1;32m--> 573\u001B[0m     \u001B[39mreturn\u001B[39;00m module\u001B[39m.\u001B[39;49mto(\u001B[39m\"\u001B[39;49m\u001B[39mcpu\u001B[39;49m\u001B[39m\"\u001B[39;49m)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1141\u001B[0m         \u001B[39mreturn\u001B[39;00m t\u001B[39m.\u001B[39mto(device, dtype \u001B[39mif\u001B[39;00m t\u001B[39m.\u001B[39mis_floating_point() \u001B[39mor\u001B[39;00m t\u001B[39m.\u001B[39mis_complex() \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                     non_blocking, memory_format\u001B[39m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1143\u001B[0m     \u001B[39mreturn\u001B[39;00m t\u001B[39m.\u001B[39mto(device, dtype \u001B[39mif\u001B[39;00m t\u001B[39m.\u001B[39mis_floating_point() \u001B[39mor\u001B[39;00m t\u001B[39m.\u001B[39mis_complex() \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1145\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_apply(convert)\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_apply\u001B[39m(\u001B[39mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[39mfor\u001B[39;00m module \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         module\u001B[39m.\u001B[39;49m_apply(fn)\n\u001B[0;32m    799\u001B[0m     \u001B[39mdef\u001B[39;00m \u001B[39mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[39mif\u001B[39;00m torch\u001B[39m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[39m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[39m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[39m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[39m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_apply\u001B[39m(\u001B[39mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[39mfor\u001B[39;00m module \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         module\u001B[39m.\u001B[39;49m_apply(fn)\n\u001B[0;32m    799\u001B[0m     \u001B[39mdef\u001B[39;00m \u001B[39mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[39mif\u001B[39;00m torch\u001B[39m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[39m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[39m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[39m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[39m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping similar frames: Module._apply at line 797 (2 times)]\u001B[0m\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_apply\u001B[39m(\u001B[39mself\u001B[39m, fn):\n\u001B[0;32m    796\u001B[0m     \u001B[39mfor\u001B[39;00m module \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mchildren():\n\u001B[1;32m--> 797\u001B[0m         module\u001B[39m.\u001B[39;49m_apply(fn)\n\u001B[0;32m    799\u001B[0m     \u001B[39mdef\u001B[39;00m \u001B[39mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    800\u001B[0m         \u001B[39mif\u001B[39;00m torch\u001B[39m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    801\u001B[0m             \u001B[39m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    802\u001B[0m             \u001B[39m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    807\u001B[0m             \u001B[39m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    808\u001B[0m             \u001B[39m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    816\u001B[0m \u001B[39m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    817\u001B[0m \u001B[39m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    818\u001B[0m \u001B[39m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    819\u001B[0m \u001B[39mwith\u001B[39;00m torch\u001B[39m.\u001B[39mno_grad():\n\u001B[1;32m--> 820\u001B[0m     param_applied \u001B[39m=\u001B[39m fn(param)\n\u001B[0;32m    821\u001B[0m should_use_set_data \u001B[39m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    822\u001B[0m \u001B[39mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32mc:\\Users\\Alexey\\anaconda3\\envs\\hands-tracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1140\u001B[0m \u001B[39mif\u001B[39;00m convert_to_format \u001B[39mis\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mand\u001B[39;00m t\u001B[39m.\u001B[39mdim() \u001B[39min\u001B[39;00m (\u001B[39m4\u001B[39m, \u001B[39m5\u001B[39m):\n\u001B[0;32m   1141\u001B[0m     \u001B[39mreturn\u001B[39;00m t\u001B[39m.\u001B[39mto(device, dtype \u001B[39mif\u001B[39;00m t\u001B[39m.\u001B[39mis_floating_point() \u001B[39mor\u001B[39;00m t\u001B[39m.\u001B[39mis_complex() \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m,\n\u001B[0;32m   1142\u001B[0m                 non_blocking, memory_format\u001B[39m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1143\u001B[0m \u001B[39mreturn\u001B[39;00m t\u001B[39m.\u001B[39;49mto(device, dtype \u001B[39mif\u001B[39;49;00m t\u001B[39m.\u001B[39;49mis_floating_point() \u001B[39mor\u001B[39;49;00m t\u001B[39m.\u001B[39;49mis_complex() \u001B[39melse\u001B[39;49;00m \u001B[39mNone\u001B[39;49;00m, non_blocking)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "paths = os.listdir(input_folder)\n",
    "random.shuffle(paths)\n",
    "\n",
    "images = [Image.open(input_folder + path) for path in paths]\n",
    "generated_images, couple = [], []\n",
    "\n",
    "for i in range(len(images)):\n",
    "    for j in range(i + 1, len(images)):\n",
    "        images[i], images[j] = images[i].resize((512, 512)), images[j].resize((512, 512))\n",
    "        generated_images.extend([\n",
    "            make_image_grid([*create_img(images[i], images[j]),\n",
    "                             images[i], images[j]], 3, 2),\n",
    "            make_image_grid([*create_img(images[j], images[i]),\n",
    "                             images[j], images[i]], 3, 2)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:26:55.018885Z",
     "start_time": "2023-10-22T00:26:52.264752Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_images(generated_images, path=output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T00:26:55.034885Z",
     "start_time": "2023-10-22T00:26:55.019883Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
